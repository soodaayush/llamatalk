# LlamaTalk

![llamatalk](https://github.com/user-attachments/assets/d2fdb3f1-6871-4da9-8c0c-b210b9ba8be3)

## Inspiration

I wanted to create my own local chatbot with an interface. When I discovered that it wasn't that hard due to an easy-to-use service called Ollama, I decided to experiment with making a local chatbot website.

## Challenges

Connecting the LLM to the website itself and ensuring the correct objects were used within the response object.

## Lessons Learned

I learned about using the Ollama service and how to send and fetch data from the LLM.

## The Website

A chatbot powered by Meta's Codellama. You can ask it a question, and you will get a response. As of now, you must run the LLM locally on your machine through a service called Ollama. From there, your LLM will be hosted on a localhost server, which you can define in the JavaScript file. LlamaTalk uses the localhost to provide a prompt and receive a response. You can customize the code to use any model with any parameter count as you please.

## Links

Ollama: https://ollama.com/
